# AI íŒŒì´í”„ë¼ì¸ ê°€ì´ë“œ

## ê°œìš”

ë¡œì»¬ LLM(Ollama)ì„ í™œìš©í•œ ì‹œë§¨í‹± ê²€ìƒ‰ê³¼ ìë™ ì™„ì„± ê¸°ëŠ¥ êµ¬í˜„ ê°€ì´ë“œ.

## ì•„í‚¤í…ì²˜ ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RecallSnippet                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   ìŠ¤ë‹ˆí«    â”‚â”€â”€â”€â”€â–¶â”‚   ì„ë² ë”©    â”‚â”€â”€â”€â”€â–¶â”‚   SQLite    â”‚   â”‚
â”‚  â”‚   ì €ì¥      â”‚     â”‚   ìƒì„±      â”‚     â”‚  + sqlite-vecâ”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                    â”‚           â”‚
â”‚                            â–¼                    â”‚           â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚           â”‚
â”‚                      â”‚   Ollama    â”‚            â”‚           â”‚
â”‚                      â”‚  (ë¡œì»¬ LLM) â”‚            â”‚           â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚           â”‚
â”‚                            â–²                    â”‚           â”‚
â”‚                            â”‚                    â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   ê²€ìƒ‰      â”‚â”€â”€â”€â”€â–¶â”‚   ì¿¼ë¦¬      â”‚â”€â”€â”€â”€â–¶â”‚   ë²¡í„°      â”‚   â”‚
â”‚  â”‚   ìš”ì²­      â”‚     â”‚   ì„ë² ë”©    â”‚     â”‚   ìœ ì‚¬ë„    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Ollama ì„¤ì •

### í•„ìš”í•œ ëª¨ë¸

```bash
# LLM (í…ìŠ¤íŠ¸ ìƒì„±ìš©) - íƒ 1
ollama pull llama3.2:3b        # ê°€ë²¼ì›€, 8GB RAM OK
ollama pull mistral:7b         # ë” ê°•ë ¥, 16GB RAM ê¶Œì¥

# ì„ë² ë”© ëª¨ë¸ (í•„ìˆ˜)
ollama pull nomic-embed-text   # 768 ì°¨ì›, ë¹ ë¦„
# ë˜ëŠ”
ollama pull mxbai-embed-large  # 1024 ì°¨ì›, ë” ì •í™•
```

### Ollama API ì—”ë“œí¬ì¸íŠ¸

```
Base URL: http://localhost:11434

POST /api/generate      # í…ìŠ¤íŠ¸ ìƒì„±
POST /api/embeddings    # ì„ë² ë”© ìƒì„±
GET  /api/tags          # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
POST /api/chat          # ëŒ€í™”í˜• (í•„ìš”ì‹œ)
```

## ì„ë² ë”© íŒŒì´í”„ë¼ì¸

### 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

```rust
fn prepare_text_for_embedding(snippet: &Snippet) -> String {
    // ì œëª© + ë¬¸ì œ + í•´ê²°ì±…ì„ ê²°í•©
    // ê°€ì¤‘ì¹˜: ì œëª©ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
    format!(
        "{title} {title} {problem} {solution}",
        title = snippet.title,
        problem = snippet.problem,
        solution = snippet.solution.as_deref().unwrap_or("")
    )
}
```

### 2. ì„ë² ë”© ìƒì„± (Rust)

```rust
use reqwest::Client;
use serde::{Deserialize, Serialize};

#[derive(Serialize)]
struct EmbeddingRequest {
    model: String,
    prompt: String,
}

#[derive(Deserialize)]
struct EmbeddingResponse {
    embedding: Vec<f32>,
}

pub async fn create_embedding(text: &str, model: &str) -> Result<Vec<f32>, AppError> {
    let client = Client::new();

    let response = client
        .post("http://localhost:11434/api/embeddings")
        .json(&EmbeddingRequest {
            model: model.to_string(),
            prompt: text.to_string(),
        })
        .send()
        .await?
        .json::<EmbeddingResponse>()
        .await?;

    Ok(response.embedding)
}
```

### 3. ì„ë² ë”© ì €ì¥

```rust
pub async fn save_embedding(
    conn: &Connection,
    snippet_id: &str,
    embedding: &[f32],
    model: &str,
) -> Result<(), AppError> {
    // f32 ë²¡í„°ë¥¼ BLOBìœ¼ë¡œ ë³€í™˜
    let embedding_bytes: Vec<u8> = embedding
        .iter()
        .flat_map(|f| f.to_le_bytes())
        .collect();

    conn.execute(
        "INSERT OR REPLACE INTO embeddings (snippet_id, embedding, embedding_model, created_at)
         VALUES (?1, ?2, ?3, datetime('now'))",
        params![snippet_id, embedding_bytes, model],
    )?;

    Ok(())
}
```

## ì‹œë§¨í‹± ê²€ìƒ‰ êµ¬í˜„

### sqlite-vec ì‚¬ìš©

```rust
use rusqlite::Connection;

pub fn setup_vector_search(conn: &Connection) -> Result<(), AppError> {
    // sqlite-vec í™•ì¥ ë¡œë“œ
    conn.load_extension_enable()?;
    conn.load_extension("vec0", None)?;
    conn.load_extension_disable()?;

    // ê°€ìƒ í…Œì´ë¸” ìƒì„± (í•œ ë²ˆë§Œ)
    conn.execute(
        "CREATE VIRTUAL TABLE IF NOT EXISTS vec_embeddings USING vec0(
            embedding float[768]  -- nomic-embed-text ì°¨ì›
        )",
        [],
    )?;

    Ok(())
}
```

### ìœ ì‚¬ë„ ê²€ìƒ‰

```rust
pub async fn semantic_search(
    conn: &Connection,
    query: &str,
    limit: usize,
    model: &str,
) -> Result<Vec<SearchResult>, AppError> {
    // 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    let query_embedding = create_embedding(query, model).await?;
    let query_bytes: Vec<u8> = query_embedding
        .iter()
        .flat_map(|f| f.to_le_bytes())
        .collect();

    // 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ê²€ìƒ‰
    let mut stmt = conn.prepare(
        "SELECT
            e.snippet_id,
            s.title,
            s.created_at,
            vec_distance_cosine(e.embedding, ?1) as distance
         FROM embeddings e
         JOIN snippets s ON e.snippet_id = s.id
         ORDER BY distance ASC
         LIMIT ?2"
    )?;

    let results = stmt
        .query_map(params![query_bytes, limit], |row| {
            Ok(SearchResult {
                snippet_id: row.get(0)?,
                title: row.get(1)?,
                created_at: row.get(2)?,
                score: 1.0 - row.get::<_, f64>(3)?, // distance â†’ similarity
            })
        })?
        .collect::<Result<Vec<_>, _>>()?;

    Ok(results)
}
```

## AI ìë™ ì™„ì„±

### í•´ê²°ì±… ìƒì„±

```rust
#[derive(Serialize)]
struct GenerateRequest {
    model: String,
    prompt: String,
    stream: bool,
}

#[derive(Deserialize)]
struct GenerateResponse {
    response: String,
}

pub async fn generate_solution(problem: &str, model: &str) -> Result<String, AppError> {
    let prompt = format!(
        r#"ë‹¹ì‹ ì€ ê°œë°œìë¥¼ ë•ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê°œë°œ ë¬¸ì œì— ëŒ€í•œ í•´ê²° ë°©ë²•ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë¬¸ì œ:
{problem}

í•´ê²° ë°©ë²•:"#,
        problem = problem
    );

    let client = Client::new();
    let response = client
        .post("http://localhost:11434/api/generate")
        .json(&GenerateRequest {
            model: model.to_string(),
            prompt,
            stream: false,
        })
        .send()
        .await?
        .json::<GenerateResponse>()
        .await?;

    Ok(response.response)
}
```

### íƒœê·¸ ì¶”ì²œ

```rust
pub async fn suggest_tags(content: &str, model: &str) -> Result<Vec<String>, AppError> {
    let prompt = format!(
        r#"ë‹¤ìŒ ê°œë°œ ê´€ë ¨ ë‚´ìš©ì„ ë¶„ì„í•˜ê³ , ì ì ˆí•œ íƒœê·¸ 3-5ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
íƒœê·¸ëŠ” ê¸°ìˆ  ìŠ¤íƒ, ì¹´í…Œê³ ë¦¬, ì£¼ìš” ê°œë… ë“±ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

ë‚´ìš©:
{content}

íƒœê·¸ (JSON ë°°ì—´ë§Œ):"#,
        content = content
    );

    let client = Client::new();
    let response = client
        .post("http://localhost:11434/api/generate")
        .json(&GenerateRequest {
            model: model.to_string(),
            prompt,
            stream: false,
        })
        .send()
        .await?
        .json::<GenerateResponse>()
        .await?;

    // JSON íŒŒì‹± (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)
    let tags: Vec<String> = serde_json::from_str(&response.response)
        .unwrap_or_else(|_| vec![]);

    Ok(tags)
}
```

## í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™

### AI ìƒì„± UI íŒ¨í„´

```tsx
function SnippetForm() {
  const [problem, setProblem] = useState("");
  const [solution, setSolution] = useState("");
  const [isGenerating, setIsGenerating] = useState(false);

  const handleGenerateSolution = async () => {
    if (!problem.trim()) return;

    setIsGenerating(true);
    try {
      const generated = await aiApi.generateSolution(problem);
      setSolution(generated);
    } catch (error) {
      toast.error("AI ìƒì„± ì‹¤íŒ¨");
    } finally {
      setIsGenerating(false);
    }
  };

  return (
    <form>
      <Textarea
        label="ë¬¸ì œ ìƒí™©"
        value={problem}
        onChange={(e) => setProblem(e.target.value)}
      />

      <div className="flex items-center gap-2">
        <label>í•´ê²° ë°©ë²•</label>
        <Button
          type="button"
          variant="outline"
          size="sm"
          onClick={handleGenerateSolution}
          disabled={isGenerating || !problem.trim()}
        >
          {isGenerating ? <Spinner /> : "ğŸ¤– AI ë„ì›€ë°›ê¸°"}
        </Button>
      </div>

      <Textarea
        value={solution}
        onChange={(e) => setSolution(e.target.value)}
        placeholder="AIê°€ ìƒì„±í•˜ê±°ë‚˜ ì§ì ‘ ì‘ì„±í•˜ì„¸ìš”..."
      />
    </form>
  );
}
```

### ì‹œë§¨í‹± ê²€ìƒ‰ í›…

```tsx
import { useState, useDeferredValue } from "react";
import { useQuery } from "@tanstack/react-query";
import { searchApi } from "@/lib/tauri";

export function useSemanticSearch(query: string) {
  const deferredQuery = useDeferredValue(query);

  return useQuery({
    queryKey: ["search", deferredQuery],
    queryFn: () => searchApi.semantic(deferredQuery, 10),
    enabled: deferredQuery.length > 2, // ìµœì†Œ 3ê¸€ì
    staleTime: 1000 * 60, // 1ë¶„ ìºì‹œ
  });
}
```

## ì„±ëŠ¥ ìµœì í™”

### ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬

```rust
// ì—¬ëŸ¬ ìŠ¤ë‹ˆí«ì„ í•œ ë²ˆì— ì„ë² ë”© (ì´ˆê¸° ì¸ë±ì‹± ì‹œ)
pub async fn batch_create_embeddings(
    snippets: &[Snippet],
    model: &str,
) -> Result<Vec<(String, Vec<f32>)>, AppError> {
    let mut results = Vec::new();

    for snippet in snippets {
        let text = prepare_text_for_embedding(snippet);
        let embedding = create_embedding(&text, model).await?;
        results.push((snippet.id.clone(), embedding));
    }

    Ok(results)
}
```

### ê²€ìƒ‰ ê²°ê³¼ ìºì‹±

```typescript
// TanStack Queryì˜ staleTime í™œìš©
const { data: results } = useQuery({
  queryKey: ["search", query],
  queryFn: () => searchApi.semantic(query, 10),
  staleTime: 1000 * 60 * 5, // 5ë¶„ ìºì‹œ
  gcTime: 1000 * 60 * 10, // 10ë¶„ í›„ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
});
```

## ì—ëŸ¬ ì²˜ë¦¬

### Ollama ì—°ê²° ì‹¤íŒ¨

```rust
pub async fn check_ollama_connection() -> Result<bool, AppError> {
    let client = Client::new();

    match client.get("http://localhost:11434/api/tags").send().await {
        Ok(response) => Ok(response.status().is_success()),
        Err(_) => Ok(false),
    }
}
```

### í”„ë¡ íŠ¸ì—”ë“œ ì—ëŸ¬ UI

```tsx
function SearchResults({ query }: { query: string }) {
  const { data, isLoading, error } = useSemanticSearch(query);

  if (error) {
    // Ollama ì—°ê²° ì‹¤íŒ¨ ì‹œ ì•ˆë‚´
    if (error.message.includes("connection")) {
      return (
        <Alert variant="warning">
          <p>AI ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ë ¤ë©´ Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.</p>
          <code>ollama serve</code>
        </Alert>
      );
    }
    return <Alert variant="error">ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.</Alert>;
  }

  // ...
}
```

## í…ŒìŠ¤íŠ¸

### ì„ë² ë”© í…ŒìŠ¤íŠ¸

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_create_embedding() {
        // Ollamaê°€ ì‹¤í–‰ ì¤‘ì¼ ë•Œë§Œ í…ŒìŠ¤íŠ¸
        if !check_ollama_connection().await.unwrap_or(false) {
            eprintln!("Skipping test: Ollama not running");
            return;
        }

        let text = "Docker container networking issue";
        let embedding = create_embedding(text, "nomic-embed-text").await.unwrap();

        assert_eq!(embedding.len(), 768);  // nomic-embed-text ì°¨ì›
        assert!(embedding.iter().all(|&v| v.is_finite()));
    }

    #[tokio::test]
    async fn test_similar_texts_have_similar_embeddings() {
        if !check_ollama_connection().await.unwrap_or(false) {
            return;
        }

        let text1 = "How to fix Docker network issue";
        let text2 = "Docker container networking problem solution";
        let text3 = "Best pizza recipes";

        let emb1 = create_embedding(text1, "nomic-embed-text").await.unwrap();
        let emb2 = create_embedding(text2, "nomic-embed-text").await.unwrap();
        let emb3 = create_embedding(text3, "nomic-embed-text").await.unwrap();

        let sim_1_2 = cosine_similarity(&emb1, &emb2);
        let sim_1_3 = cosine_similarity(&emb1, &emb3);

        // ë¹„ìŠ·í•œ í…ìŠ¤íŠ¸ê°€ ë” ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì ¸ì•¼ í•¨
        assert!(sim_1_2 > sim_1_3);
    }
}
```
